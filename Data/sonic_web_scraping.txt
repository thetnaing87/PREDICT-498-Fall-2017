#!/usr/bin/env python2# -*- coding: utf-8 -*-"""Created on Thu Oct  5 06:21:36 2017@author: Jessica"""import bs4import pandas as pdimport urllibfrom urllib import urlopen as uReq#from urlopen.request import urlopen as uReq from bs4 import BeautifulSoup as soupimport requestsimport timecommentsLst=[]ratingLst = []ratingDateLst = []urlLst = []#open connection with the urldef open_connection(url):        uClient = uReq(url)    page_html = uClient.read() # read html page    uClient.close() # close connection after fetching the page    page_soup = soup(page_html, "html.parser") # parsing HTML with beautiful soup     return page_soup    # get url of all the pages with Sonic Denver, co reviews and ratings    def get_page_url(url):     uClient = uReq(url)   page_html = uClient.read()   uClient.close()   page_soup = soup(page_html, "html.parser")      link=page_soup.findAll("div",{"class":"arrange_unit page-option"}) #find the block which contain tags of all the pages that    #have reviews and ratings    #loop throught the tags and extract url from tag 'a' with 'href'   for i in link:       u=(i.a['href'])        url=u       urlLst.append(url) #append all the urls in the list   return urlLst      #go to url of each page and extract reviews, review date, and ratings     def get_contents(url):         uClient = uReq(url)    page_html = uClient.read()            uClient.close()    page_soup = soup(page_html, "html.parser")     time.sleep(2)                container = page_soup.findAll("div", {"class": "review-content"}) #extract the the tag of the block that     #contains all the elements which store reviews , date and rating            # look through t container and get for tags of elements that distplay date, reviews and     #rating. The tags are indetified on the page using inspect. Store the extracted data in three different lists    for c in container:                 list1=[]                 list2=[]                 list3=[]                 print '*****************************************************'                 ratng = c.div.div.div['title']                  print(ratng)                 ratingdate = c.find('span').getText()                 print(ratingdate)                 contents =  c.p.getText()                 print contents                 #strip extra spaces from the extracted data                 rating  = ''.join(ratng).strip()                   rating_date= ''.join(ratingdate).strip()                 comments = ''.join(contents).strip()                #append extracted data to the list                 list1.append(rating_date)                  list2.append(rating)                 list3.append(comments)                         ratingDateLst.append(list1)                 ratingLst.append(list2)                 commentsLst.append(list3)     #store each list in one dataframe                 df1 = pd.DataFrame({'Rating_Date':ratingDateLst})    print(df1)    df2 = pd.DataFrame({'Rating_List':ratingLst})    print(df2)    df3 = pd.DataFrame({'Comments':commentsLst})    print(df3)    frames = [df1, df2, df3]    rating_df = pd.concat(frames, axis = 1)     #write dataframe to the out file    rating_df.to_csv("output.csv", sep=",",  encoding='utf-8')                def main(): #call to the open connection to get the html page of the first url of Sonic, Denver url="https://www.yelp.com/biz/sonic-denver?hrid=ryzsw2fbRMqRdpmwaFpSZA&osq=Sonic+Drive-In" page_soup = open_connection(url)  #get the url of all the pages from the first yelp page of sonic  lst =  get_page_url(url)  get_contents(url)  #get the html page of all the subsiquent pages and get reviews, review date and contents for i in lst:       get_contents(i)       if __name__ == '__main__': main()               